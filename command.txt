# training QA
python run_qa.py \
--model_name_or_path hfl/chinese-roberta-wwm-ext \
--do_train \
--do_eval \
--learning_rate 5e-5 \
--num_train_epochs 1 \
--output_dir /tmp2/b09902123/hw2/checkpoint_qa/ \
--per_gpu_eval_batch_size=2 \
--per_device_train_batch_size=2 \
--max_seq_length 512 \
--cache_dir /tmp2/b09902123/hw2/cache_qa/ \
--pad_to_max_length \
--overwrite_output \
--train_file /tmp2/b09902123/hw2/train.json \
--validation_file /tmp2/b09902123/hw2/valid.json \
--context_file /tmp2/b09902123/hw2/context.json \
--gradient_accumulation_steps 8 \
--max_train_samples 10 \
--max_eval_samples 10

# predict QA
python run_qa.py \
--model_name_or_path /tmp2/b09902123/hw2/checkpoint_qa/ \
--do_predict \
--learning_rate 5e-5 \
--num_train_epochs 1 \
--output_dir /tmp2/b09902123/hw2/checkpoint_qa/ \
--per_gpu_eval_batch_size=2 \
--per_device_train_batch_size=2 \
--max_seq_length 512 \
--cache_dir /tmp2/b09902123/hw2/cache_qa/ \
--pad_to_max_length \
--test_file /tmp2/b09902123/hw2/result.json \
--context_file /tmp2/b09902123/hw2/context.json \
--output_file /tmp2/b09902123/hw2/answer.csv \
--gradient_accumulation_steps 8 \
--max_predict_samples 3

===============================================================================================

# training multiple choice
python train_multiple.py \
--model_name_or_path bert-base-chinese \
--do_train \
--do_eval \
--learning_rate 5e-5 \
--num_train_epochs 1 \
--output_dir /tmp2/b09902123/hw2/checkpoint/ \
--per_gpu_eval_batch_size=2 \
--per_device_train_batch_size=2 \
--max_seq_length 512 \
--cache_dir /tmp2/b09902123/hw2/cache/ \
--pad_to_max_length \
--overwrite_output \
--train_file /tmp2/b09902123/hw2/train.json \
--validation_file /tmp2/b09902123/hw2/valid.json \
--context_file /tmp2/b09902123/hw2/context.json \
--gradient_accumulation_steps 8 \
--max_train_samples 1000

# predict multiple choice
python train_multiple.py \
--model_name_or_path /tmp2/b09902123/hw2/checkpoint/ \
--do_predict \
--learning_rate 5e-5 \
--num_train_epochs 1 \
--output_dir /tmp2/b09902123/hw2/checkpoint/ \
--per_gpu_eval_batch_size=2 \
--per_device_train_batch_size=2 \
--max_seq_length 512 \
--cache_dir /tmp2/b09902123/hw2/cache/ \
--pad_to_max_length \
--test_file /tmp2/b09902123/hw2/test.json \
--output_file /tmp2/b09902123/hw2/result.json \
--context_file /tmp2/b09902123/hw2/context.json \
--gradient_accumulation_steps 8